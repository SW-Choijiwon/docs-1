
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Hive 테이블 - Spark 2.4.3 Documentation</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html">
                      <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">2.4.3</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">개요</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">프로그래밍 가이드<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">빠른 시작</a></li>
                                <li><a href="sql-programming-guide.html">스파크 SQL, DataFrame, Dataset</a></li>
                                <li><a href="structured-streaming-programming-guide.html">구조적 스트리밍</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API 문서<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package">Scala</a></li>
                                <li><a href="https://spark.apache.org/docs/latest/api/java/index.html">Java</a></li>
                                <li><a href="https://spark.apache.org/docs/latest/api/python/index.html">Python</a></li>
                                <li><a href="https://spark.apache.org/docs/latest/api/R/index.html">R</a></li>
                                <li><a href="https://spark.apache.org/docs/latest/api/sql/index.html">Spark SQL 함수</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.3</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="sql-programming-guide.html">Spark SQL Guide</a></h3>
        
<ul>

    <li>
        <a href="sql-getting-started.html">
            
                시작하기
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources.html">
            
                데이터 소스
            
        </a>
    </li>
    
    
        
<ul>

    <li>
        <a href="sql-data-sources-load-save-functions.html">
            
                일반 불러오기/저장하기 함수
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-parquet.html">
            
                Parquet 파일
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-orc.html">
            
                ORC 파일
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-json.html">
            
                JSON 파일
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-hive-tables.html">
            
                <b>Hive 테이블</b>
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-jdbc.html">
            
                JDBC를 통한 다른 데이터베이스 사용하기
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-avro.html">
            
                Avro 파일
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-troubleshooting.html">
            
                문제 해결
            
        </a>
    </li>
    
    

</ul>

    

    <li>
        <a href="sql-performance-tuning.html">
            
                성능 튜닝
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-distributed-sql-engine.html">
            
                분산 SQL 엔진
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html">
            
                아파치 애로우(Arrow)와 Pandas를 위한 PySpark 사용 가이드
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-reference.html">
            
                참조
            
        </a>
    </li>
    
    

</ul>

    </div>
</div>
                
                <input id="nav-trigger" class="nav-trigger" checked type="checkbox">
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar" id="content">
                    
                        <h1 class="title">Hive 테이블</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#hive-테이블의-저장-형식-지정하기" id="markdown-toc-hive-테이블의-저장-형식-지정하기">Hive 테이블의 저장 형식 지정하기</a></li>
  <li><a href="#서로-다른-버전의-hive-메타스토어와-연동하기" id="markdown-toc-서로-다른-버전의-hive-메타스토어와-연동하기">서로 다른 버전의 Hive 메타스토어와 연동하기</a></li>
</ul>

<p>스파크 SQL은 Apache Hive에 저장된 데이터에 대한 읽기/쓰기를 지원합니다. Hive가 이미 많은 의존 라이브러리를 포함하고 있기때문에, 기본 스파크 배포판은 이 의존 라이브러리를 포함하고 있지 않습니다. Hive의 의존성 라이브러리를 classpath에서 찾을 수 있으면, 스파크는 이를 자동으로 로드합니다. 모든 작업 노드가 Hive에 저장된 데이터에 접근하기 위해 Hive 직렬화/역직렬화 라이브러리(SerDe) 그리고 Hive 의존 라이브러리는 모든 작업 노드에서 접근 가능해야 합니다.</p>

<p>Hive 관련 설정을 하기 위해서는 conf/ 안에 <code>hive-site.xml</code>, <code>core-site.xml</code>(보안 설정용)과 <code>hdfs-site.xml</code>(HDFS 설정용)파일을 넣어 주면 됩니다.</p>

<p>Hive를 사용할 때, SparkSession를 객체에 지속되는 Hive 메타스토어로의 연결성, Hive SerDe, Hive 사용자 정의 함수 등의 기능을 설정할 수 있습니다. Hive 배포판이 설치되어 있지 않더라도 Hive 지원을 활성화할 수 있습니다. <code>hive-site.xml</code>이 설정되어 있지 않은 경우, 현재 디렉토리에서 <code>metastore_db</code>를 자동으로 생성하고 <code>spark.sql.warehouse.dir</code>에 설정된 디렉토리를 생성합니다. <code>spark-warehouse</code>의 기본 디렉토리는 스파크 애플리케이션을 시작한 현재 디렉토리입니다. <code>hive-site.xml</code>의 <code>hive.metastore.warehouse.dir </code>속성은 스파크 2.0.0 버전부터 더 이상 지원되지 않으며, 대신 warehouse에서 데이터베이스의 기본 위치를 명시하려면 <code>spark.sql.warehouse.dir</code>을 사용해야 합니다. 스파크 애플리케이션을 실행하는 유저에게 쓰기 권한의 승인이 필요할 수 있습니다.</p>

<div class="codetabs">

<div data-lang="scala">
    <div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">java.io.File</span>

<span class="k">import</span> <span class="nn">org.apache.spark.sql.</span><span class="o">{</span><span class="nc">Row</span><span class="o">,</span> <span class="nc">SaveMode</span><span class="o">,</span> <span class="nc">SparkSession</span><span class="o">}</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">Record</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="c1">// warehouseLocation은 hive 데이터베이스와 테이블의 기본 위치를 지정합니다.</span>
<span class="k">val</span> <span class="n">warehouseLocation</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">File</span><span class="o">(</span><span class="s">&quot;spark-warehouse&quot;</span><span class="o">).</span><span class="n">getAbsolutePath</span>

<span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span>
  <span class="o">.</span><span class="n">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="n">appName</span><span class="o">(</span><span class="s">&quot;Spark Hive Example&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">config</span><span class="o">(</span><span class="s">&quot;spark.sql.warehouse.dir&quot;</span><span class="o">,</span> <span class="n">warehouseLocation</span><span class="o">)</span>
  <span class="o">.</span><span class="n">enableHiveSupport</span><span class="o">()</span>
  <span class="o">.</span><span class="n">getOrCreate</span><span class="o">()</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">spark.sql</span>

<span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="o">)</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="o">)</span>

<span class="c1">// HiveQL로 쿼리를 표현합니다. (역자 주: Spark SQL과 조금 다릅니다.)</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM src&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |key|  value|</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |238|val_238|</span>
<span class="c1">// | 86| val_86|</span>
<span class="c1">// |311|val_311|</span>
<span class="c1">// ...</span>

<span class="c1">// 집계 쿼리도 지원합니다.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------+</span>
<span class="c1">// |count(1)|</span>
<span class="c1">// +--------+</span>
<span class="c1">// |    500 |</span>
<span class="c1">// +--------+</span>

<span class="c1">// SQL 쿼리의 결과는 그 자체로 DataFrame이며, 모든 일반적인 함수를 지원합니다.</span>
<span class="k">val</span> <span class="n">sqlDF</span> <span class="k">=</span> <span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="o">)</span>

<span class="c1">// DataFrame은 Row 타입으로 이루어져 있습니다. 각 컬럼에는 index를 사용해서 접근할 수 있습니다.</span>
<span class="k">val</span> <span class="n">stringsDS</span> <span class="k">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">Row</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="s">s&quot;Key: </span><span class="si">$key</span><span class="s">, Value: </span><span class="si">$value</span><span class="s">&quot;</span>
<span class="o">}</span>
<span class="n">stringsDS</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |               value|</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// ...</span>

<span class="c1">// DataFrame을 사용하여 SparkSession에 임시 뷰를 생성할 수 있습니다.</span>
<span class="k">val</span> <span class="n">recordsDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="o">((</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">100</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="nc">Record</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="s">s&quot;val_</span><span class="si">$i</span><span class="s">&quot;</span><span class="o">)))</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;records&quot;</span><span class="o">)</span>

<span class="c1">// 이제, DataFrame의 데이터와 Hive에 저장된 데이터에 JOIN 쿼리를 사용할 수 있습니다.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |key| value|key| value|</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |  2| val_2|  2| val_2|</span>
<span class="c1">// |  4| val_4|  4| val_4|</span>
<span class="c1">// |  5| val_5|  5| val_5|</span>
<span class="c1">// ...</span>

<span class="c1">// `USING hive`를 사용하는 스파크 SQL 문법 대신 HiveQL의 문법을 사용하여 Hive managed Parquet 테이블을 생성합니다.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;</span><span class="o">)</span>
<span class="c1">// DataFrame을 Hive managed 테이블로 저장합니다.</span>
<span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="o">(</span><span class="s">&quot;src&quot;</span><span class="o">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="o">(</span><span class="nc">SaveMode</span><span class="o">.</span><span class="nc">Overwrite</span><span class="o">).</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;hive_records&quot;</span><span class="o">)</span>
<span class="c1">// 데이터를 삽입한 후에는 Hive 매니지드 테이블(managed table)에 데이터가 저장됩니다.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM hive_records&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |key|  value|</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |238|val_238|</span>
<span class="c1">// | 86| val_86|</span>
<span class="c1">// |311|val_311|</span>
<span class="c1">// ...</span>

<span class="c1">// Parquet 데이터 디렉토리를 지정합니다.</span>
<span class="k">val</span> <span class="n">dataDir</span> <span class="k">=</span> <span class="s">&quot;/tmp/parquet_data&quot;</span>
<span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="o">(</span><span class="n">dataDir</span><span class="o">)</span>
<span class="c1">// Hive 외부 Parquet 테이블을 생성합니다.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">s&quot;CREATE EXTERNAL TABLE hive_ints(key int) STORED AS PARQUET LOCATION &#39;</span><span class="si">$dataDir</span><span class="s">&#39;&quot;</span><span class="o">)</span>
<span class="c1">// Hive 외부 테이블은 이미 데이터를 갖고있어야 합니다.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM hive_ints&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+</span>
<span class="c1">// |key|</span>
<span class="c1">// +---+</span>
<span class="c1">// |  0|</span>
<span class="c1">// |  1|</span>
<span class="c1">// |  2|</span>
<span class="c1">// ...</span>

<span class="c1">// Hive 동적 파티셔닝(Partitioning)을 위한 플래그를 활성화합니다.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;hive.exec.dynamic.partition&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;hive.exec.dynamic.partition.mode&quot;</span><span class="o">,</span> <span class="s">&quot;nonstrict&quot;</span><span class="o">)</span>
<span class="c1">// DataFrame API를 사용하여 Hive 분할 테이블을 생성합니다.</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">partitionBy</span><span class="o">(</span><span class="s">&quot;key&quot;</span><span class="o">).</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;hive&quot;</span><span class="o">).</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;hive_part_tbl&quot;</span><span class="o">)</span>
<span class="c1">// 분할된 컬럼 `key`는 스키마의 마지막 순서로 이동합니다.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM hive_part_tbl&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +-------+---+</span>
<span class="c1">// |  value|key|</span>
<span class="c1">// +-------+---+</span>
<span class="c1">// |val_238|238|</span>
<span class="c1">// | val_86| 86|</span>
<span class="c1">// |val_311|311|</span>
<span class="c1">// ...</span>

<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="o">()</span>
</pre></div>
    <div><small>스파크 저장소의 "examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala"에서 전체 예제 코드를 볼 수 있습니다.</small></div>
  </div>

<div data-lang="python">
    <div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">expanduser</span><span class="p">,</span> <span class="n">join</span><span class="p">,</span> <span class="n">abspath</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="c1"># warehouse_location은 hive 데이터베이스와 테이블의 기본 위치를 지정합니다.</span>
<span class="n">warehouse_location</span> <span class="o">=</span> <span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;spark-warehouse&#39;</span><span class="p">)</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Python Spark SQL Hive integration example&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.warehouse.dir&quot;</span><span class="p">,</span> <span class="n">warehouse_location</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span> \
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="c1"># spark는 위에서 생성한 SparkSession 객체입니다.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="p">)</span>

<span class="c1"># HiveQL로 쿼리를 표현합니다.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+-------+</span>
<span class="c1"># |key|  value|</span>
<span class="c1"># +---+-------+</span>
<span class="c1"># |238|val_238|</span>
<span class="c1"># | 86| val_86|</span>
<span class="c1"># |311|val_311|</span>
<span class="c1"># ...</span>

<span class="c1"># 집계 쿼리도 지원합니다.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +--------+</span>
<span class="c1"># |count(1)|</span>
<span class="c1"># +--------+</span>
<span class="c1"># |    500 |</span>
<span class="c1"># +--------+</span>

<span class="c1"># SQL 쿼리의 결과는 DataFrame으로 생성되며 모든 일반 함수를 지원합니다.</span>
<span class="n">sqlDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="p">)</span>

<span class="c1"># DataFrame은 Row 타입으로 이루어져 있습니다. 각 컬럼에는 index를 사용해서 접근할 수 있습니다.</span>
<span class="n">stringsDS</span> <span class="o">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="s2">&quot;Key: </span><span class="si">%d</span><span class="s2">, Value: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
<span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">stringsDS</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># ...</span>

<span class="c1"># DataFrame을 사용하여 SparkSession에 임시 뷰를 생성할 수 있습니다.</span>
<span class="n">Record</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">)</span>
<span class="n">recordsDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Record</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="s2">&quot;val_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)])</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>

<span class="c1"># 이제, DataFrame의 데이터와 Hive에 저장된 데이터에 JOIN 쿼리를 사용할 수 있습니다.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+------+---+------+</span>
<span class="c1"># |key| value|key| value|</span>
<span class="c1"># +---+------+---+------+</span>
<span class="c1"># |  2| val_2|  2| val_2|</span>
<span class="c1"># |  4| val_4|  4| val_4|</span>
<span class="c1"># |  5| val_5|  5| val_5|</span>
<span class="c1"># ...</span>
</pre></div>
    <div><small>스파크 저장소의 "examples/src/main/python/sql/hive.py"에서 전체 예제 코드를 볼 수 있습니다.</small></div>
  </div>

</div>

<h3 id="hive-테이블의-저장-형식-지정하기">Hive 테이블의 저장 형식 지정하기</h3>

<p>Hive 테이블을 생성할 때, 이 테이블이 어떻게 파일시스템에서/으로 데이터를 읽고/쓸지 정의해야 합니다. 다시 말해 &#8220;입력 형식&#8221;과 &#8220;출력 형식&#8221;을 정의해야 합니다. 또한, 이 테이블이 데이터를 로우로 역직렬화하거나 로우를 데이터로 직렬화하는 방식(serde)도 정의해야 합니다. 아래의 옵션(&#8220;serde&#8221;, &#8220;input format&#8221;, &#8220;output format&#8221;)을 사용하여 <code>CREATE TABLE src(id int) USING hive OPTIONS(fileFormat 'parquet') </code>와 같이 저장 형식을 명시할 수 있습니다. 기본적으로, 테이블 파일은 플레인 텍스트(plain text)로 읽어들입니다. 단, 테이블을 생성할 때 Hive의 스토리지 핸들러 기능은 아직 지원되지 않으므로, Hive에서 직접 저장소 핸들러를 사용하여 테이블을 생성하고 스파크 SQL에서 읽어오는 방법을 사용할 수 있습니다.</p>

<table class="table">
  <tr><th>속성 이름</th><th>의미</th></tr>
  <tr>
    <td><code>fileFormat</code></td>
    <td>fileForamat은 "serde", "input format", "output format"등과 같은 저장 형식 명세의 한 종류입니다. 현재 6가지의 fileFormat을 지원합니다: 'sequencefile', 'rcfile', 'orc', 'parquet', 'textfile', 'avro'</td>
  </tr>
  <tr>
    <td><code>inputFormat, outputFormat</code></td>
    <td>이 두 옵션은 글자 그대로 사용할 `InputFormat`과 `OutputFormat`의 이름을 지정합니다(문자열 타입).예를 들면, `org.apache.hadoop.hive.ql.io.orc.OrcInputFormat`와 같습니다. 이 두 가지 옵션은 한 쌍으로 함께 사용하며, `fileForamt` 옵션을 이미 사용하였다면 이 옵션은 사용할 수 없습니다.</td>
  </tr>
  <tr>
    <td><code>serde</code></td>
    <td>seder 클래스의 이름을 명시합니다. `fileFormat` 옵션이 이미 명시되어 있고 여기에 serde에 대한 정보가 포함되어 있다면 이 옵션을 사용할 수 없습니다. 현재, 6가지의 fileFormat 옵션 중 "sequencefile", "textfile", "rcfile" 세 가지 옵션은 serde에 대한 정보를 포함하지 않으므로, fileFormat에서 이 세 가지 옵션을 사용하고 있을 때는 이 옵션을 사용할 수 있습니다.</td>
  </tr>
  <tr>
    <td><code>fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</code></td>
    <td>fileFormat 옵션으로 "textfile"이 지정되어 있을 때만 사용가능합니다. 필드가 구분된 파일(delimited file)을 로우로 변환하는 방법을 정의합니다.</td>
  </tr>
</table>

<p><code>OPTIONS</code> 구문으로 정의되는 다른 모든 속성은 Hive serde 속성으로 간주됩니다.</p>

<h3 id="서로-다른-버전의-hive-메타스토어와-연동하기">서로 다른 버전의 Hive 메타스토어와 연동하기</h3>

<p>스파크 SQL의 Hive 지원에서 가장 중요한 부분 중 하나는, 스파크 SQL이 Hive 테이블의 메타데이터에 접근할 수 있도록 하는 Hive 메타스토어와의 연동 기능입니다. 스파크 1.4.0 버전부터, 아래에 설명된 설정을 사용하면, 단일 스파크 SQL 빌드에서 서로 다른 버전의 Hive 메타스토어에 쿼리를 실행할 수 있습니다. 연동하는 메타스토어 Hive의 버전과는 별개로, 스파크 SQL은 Hive 1.2.1 버전을 기준으로 컴파일되며 이 버전에 포함된 클래스(serde, UDF, UDAF 등)를 내부적으로 사용합니다.</p>

<p>아래의 옵션을 사용하여 메타데이터를 받아올 때 사용되는 Hive 버전을 설정할 수 있습니다:</p>

<table class="table">
  <tr><th>속성 이름</th><th>기본값</th><th>의미</th></tr>
  <tr>
    <td><code>spark.sql.hive.metastore.version</code></td>
    <td><code>1.2.1</code></td>
    <td>Hive 메타스토어의 버전. <code>0.12.0</code> 버전부터 <code>2.3.3</code> 버전까지 사용할 수 있습니다.</td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.jars</code></td>
    <td><code>builtin</code></td>
    <td>
      Hive 메타스토어에 연결할 때 사용하는 HiveMetastoreClient 객체를 생성하는데 사용될 jar 파일의 위치. 다음 세 가지 옵션이 사용 가능합니다:
      <ol>
        <li><code>builtin</code></li>
        <code>-Phive</code>이 활성화되어 있을 때 스파크에 포함되어 있는 Hive 1.2.1을 사용합니다. 이 옵션을 사용하면 <code>spark.sql.hive.metastore.version</code>는 1.2.1이 되거나 정의되지 않아야 합니다.
        <li><code>maven</code></li>
        Maven 저장소에서 명시된 버전의 Hive jar를 다운로드하여 사용합니다. 이 설정을 운영 환경(production environment)에서 사용하는 것은 추천하지 않습니다.
        <li>JVM의 표준 형식 classpath. 이 classpath는 Hive와 올바른 버전의 Hadoop을 포함한 모든 의존 라이브러리를 포함해야 합니다. 이 jar 파일은 드라이버에서 접근 가능해야하며, yarn 클러스터 모드에서 실행하고자 한다면 애플케이션으로 패키지화되어 있어야 합니다.</li>
      </ol>
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.sharedPrefixes</code></td>
    <td><code>com.mysql.jdbc,<br />org.postgresql,<br />com.microsoft.sqlserver,<br />oracle.jdbc</code></td>
    <td>
      <p>스파크 SQL과 (특정 버전의) Hive 사이에 공유되는 classloader를 사용하여 로드해야 하는 클래스들의 접두사 목록(쉼표로 구분). 예를 들면, Hive 메타스토어와 연결하는 데 사용되는 JDBC 드라이버 목록 같은 경우입니다. (역자 주: Hive의 메타스토어로는 MySQL, PostgreSQL 등의 데이터베이스를 사용할 수 있습니다. MySQL에 연결하려면 <code>com.mysql.jdbc</code> 패키지의 클래스가, PostgreSQL에 연결하려면 <code>org.postgresql</code> 패키지의 클래스를 사용해야 합니다. 각각의 경우 <code>com.mysql.jdbc</code>, <code>org.postgresql</code>가 지정되어야 합니다.) 이미 공유되고 있는 클래스와의 상호작용을 위해 필요한 클래스의 접두사들 역시 명시되어야 합니다. (예: log4j에서 사용하는 사용자 정의 Appender)
      </p>
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.barrierPrefixes</code></td>
    <td><code>(empty)</code></td>
    <td>
      <p>
        스파크 SQL이 붙는 Hive 각각의 버전에 따라 명시적으로 로드되어야 하는 클래스 접두사 목록(쉼표로 구분). 예를 들어 접두사를 공유하는 식으로 선언되는 게 보통인 Hive UDF가 여기에 포함됩니다. (예: <code>org.apache.spark.*</code>)
      </p>
    </td>
  </tr>
</table>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
